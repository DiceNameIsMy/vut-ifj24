% Author: Savin Ivan xsavini00
% Author: Turar Nurdaulet xturarn00
% Author: Krasovskyi Oleh xkrasoo00 
% Author: Popov Albert xpopov10 


\documentclass[a4paper, 11pt]{article}


\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[left=2cm, top=3cm, text={17cm, 24cm}]{geometry}
\usepackage[unicode,pdftitle={Překladač jazyka IFJ24},hidelinks]{hyperref}
\usepackage{times}
\usepackage{forest} % В преамбуле документа подключите пакет forest
\usepackage{verbatim}
\usepackage{enumitem}
\usepackage{graphicx} % vkládání obrázků
\usepackage{longtable}
\usepackage{amsmath}
\hypersetup{
	colorlinks = true,
	hypertexnames = false,
	citecolor = red
}


\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}} % makro na sázení římských čísel


\begin{document}


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Titulní stránka %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{titlepage}
		\begin{center}
			\includegraphics[width=0.77\linewidth]{FIT_logo.png} \\

			\vspace{\stretch{0.382}}

			\Huge{Project documentation} \\
			\LARGE{\textbf{Implementace překladače imperativního jazyka IFJ24}} \\
			\Large{Tým xpopov10, varianta vv-BVS}
			\vspace{\stretch{0.618}}
		\end{center}

		\begin{minipage}{0.4 \textwidth}
			{\Large \today}
		\end{minipage}
		\hfill
		\begin{minipage}[r]{0.6 \textwidth}
			\Large
			\begin{tabular}{l l l}
				\textbf{Popov Albert} & \textbf{(xpopov10)} & \quad 25\,\% \\
				Krasovskyi Oleh & (xkrasoo00) & \quad 25\,\% \\
				Savin Ivan & (xsavini00) & \quad 25\,\% \\
				Turar Nurdaulet & (xturarn00) & \quad 25\,\% \\
			\end{tabular}
		\end{minipage}
	\end{titlepage}



	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Introduction %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\pagenumbering{arabic}
	\setcounter{page}{1}

	\section{Introduction}

	The goal of the project was to create a C program that reads source code written in the IFJ24 source language,
    which is a simplified subset of the imperative programming language Zig 0.13, and translates it into the target language IFJcode24 (intermediate code).
    
    The program is a console application that reads the source program from standard input and generates
    the resulting intermediate code to the standard output or, in the event of an error, returns the corresponding error code.



    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Work in the team %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Work in the team}

	\subsection{How the team works}

    At the beginning of the project, we divided the task into 4 main blocks and acted according to the interaction of the codes. If someone's work seemed less important, that person helped others or wrote tests.
	\subsubsection{Version control system}

	We used the Git versioning system to manage the project files. As a remote repository we used \mbox{GitHub}.

	Git allowed us to work on multiple tasks on a project at the same time in so-called branches. Most of the tasks were first prepared
	into a branch, and only after testing and approving the modifications by other team members did we incorporate them into the main
	development branch.

	\subsubsection{Communication}

	Communication was mainly in a group chat and in private messages. During the first half of the semester, several offline meetings were conducted.

	Towards the end of the project, we met to demonstrate how our program blocks worked, in order to find errors and increase the understanding of the project to the other participants.


	\subsection{Distribution of work among team members}

	We divided the work on the project evenly with regard to its complexity and time demands.
	So everyone got a percentage rating of 25\,\%.
	The table summarizes the distribution of the team's work among the individual members.
	\bigskip
	\begin{table}[ht]
		\centering
		\begin{tabular}{| l | l |}
			\hline
			Team Member & Assigned Work \\ \hline
			\textbf{Popov Albert} & \begin{tabular}{l} team management, testing, syntactic analysis  \end{tabular} \\
			Krasovskyi Oleg & \begin{tabular}{l} semantic analysis, testing \end{tabular} \\
			Savin Ivan & \begin{tabular}{l}lexical analysis, documentation, testing \end{tabular} \\
			Turar Nurdaulet & \begin{tabular}{l} target code generation, work organization, testing, project structure\end{tabular} \\ \hline
		\end{tabular}
		\caption{Distribute the work of the team among the individual members}
		\label{table:rozdeleni_prace}
	\end{table}


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Návrh a implementace %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Design and implementation}

	The project is composed of several parts implemented by us, which are presented in this chapter.
	It also shows how the different subparts work together.

    \subsection{Compiler structure}
    The compiler consists of the following modules:\\
    \textbf{Lexical analyzer (lexer):} Converts source code to tokens\\
    \textbf{Parser:} It analyzes tokens using syntactic and semantic rules and then builds an abstract syntax tree (AST).\\
    \textbf{Code Generator:} Generates target code from AST.\\

	\subsection{Lexical Analysis}

        The first stage of the compiler is lexical analysis. Initially, we assumed that a separator symbol would exist between all tokens, allowing the source code to be divided into discrete blocks. Each block could then be easily converted into corresponding tokens. However, this approach was abandoned due to the need to implement numerous exceptions in certain cases, which made the lexer harder to extend and reduced code readability.
        
        The new version of the lexical analyzer employs a deterministic finite automaton (DFA). The main function, \texttt{run\_lexer()}, reads the input code and starts the automaton in its initial state. It identifies the first non-empty character and transitions the automaton to the corresponding state. After processing that state, the control returns to \texttt{run\_lexer()}, which continues reading until the end of the file.
    
        \textbf{Processing Identifiers}
    
        For identifiers, allowed characters are read sequentially. Once the sequence is fully read, the lexer determines whether it matches a keyword or should be classified as an identifier. This approach allows keywords to be correctly identified and associated tokens to be created.
    
        \textbf{Processing Numbers}
    
        For numerical tokens, digits, decimal points, and the characters \texttt{e/E} are read in sequence. If the input forms a valid number, the lexer generates an \texttt{f64} or \texttt{i32} token based on the data provided.
        
        \textbf{Finite State Machine Design}
        
        The entire lexical analyzer is implemented as a deterministic finite automaton based on a predefined state diagram (included in the appendix). This diagram defines the states and transitions, ensuring the lexer is easier to maintain and extend when required.
        
        By utilizing a structured DFA approach, the lexer achieves a balance between functionality and simplicity, making it a reliable component of the compiler pipeline.


	\subsection{Syntactic Analysis}

        The syntactic analyzer, or parser, ensures that the input source code adheres to the defined grammar of the programming language. It processes the sequence of tokens produced by the lexical analyzer and organizes them into an Abstract Syntax Tree (AST), which represents the hierarchical structure of the program.
        
        \textbf{How it works}
        
        \textbf{Input:}  
        The parser takes a sequence of tokens from the lexical analyzer. Each token consists of its type and optionally its attributes (e.g., literal values or identifier names).
        
        \textbf{Parsing Strategy:}  
        The parser uses a recursive descent parsing approach. Each grammatical rule is implemented as a specific function that matches tokens and recursively processes nested structures.
        
        \textbf{Abstract Syntax Tree (AST):}  
        The parser constructs an AST for syntactically valid input. The AST represents program constructs such as expressions, statements, and function definitions in a hierarchical form. For example, an expression like \texttt{a + b * c} is represented as a tree with \texttt{+} as the root, \texttt{a} as the left child, and a subtree for \texttt{b * c} as the right child.
        
        \textbf{Error Handling:}  
        The parser reports syntax errors if a token does not match the expected grammar rule. For example, if a semicolon is missing, the parser will report an error and gracefully terminate with a message indicating the expected token and its position.
        
        \subsubsection*{Workflow}
        
        \textbf{Initialization:}  
        Parsing begins with the \texttt{parseProgram} function, which processes the overall program structure. It first validates the prologue and then processes all function definitions.
        
        \textbf{Recursive Descent Parsing:}  
        Each grammar rule is implemented as a function (e.g., \texttt{parseExpression}, \texttt{parseStatement}). These functions match tokens and recursively call other functions to process nested constructs.
        
        \textbf{Token Matching:}  
        The \texttt{match} function ensures the current token matches the expected type. If not, a syntax error is raised.
        
        \textbf{AST Construction:}  
        For valid input, the parser creates AST nodes using helper functions like \texttt{createASTNode} and \texttt{createBinaryASTNode}. These nodes capture the syntactic structure of constructs such as expressions, statements, and function definitions.
        
        \textbf{Completion:}  
        After processing all tokens, the parser finalizes the AST, which is then passed to the semantic analyzer for further validation.
        
        
        \subsubsection*{Example Workflow}
        
        \begin{verbatim}
        const ifj = @import("ifj24.zig");
        
        pub fn main() void {
            var x: i32 = 5; 
            x = x + 1;
            return;
        }
        \end{verbatim}
        
        The parser verifies the validity of the prologue: \texttt{const ifj = @import("ifj24.zig");}.  
        It analyzes the function definition \texttt{pub fn main() void}.  
        Inside \texttt{main}, it analyzes:  
        - Variable declaration: \texttt{var x: i32 = 5}.  
        - Assignment: \texttt{x = x + 1}.  
        - Return statement: \texttt{return}.  
        
        The resulting AST has the following structure:  
        
        \begin{verbatim}
        Program
        ├── Prolog
        └── FunctionDef (main)
            ├── Parameters: None
            ├── ReturnType: void
            └── StatementList
                ├── VarDeclaration (x: i32 = 5)
                ├── Assignment (x = x + 1)
                └── Return
        \end{verbatim}
        
        The AST is then passed to the semantic analyzer.

	\subsection{Semantic Analysis}

    Semantic analysis is performed alongside syntax analysis, using AST nodes as storage units for various useful data, such as:
    \begin{itemize}
        \item Compile-time values for variables, constants, literals, and expressions.
        \item Value types for variables, constants, literals, expressions, and statements, for type-checking purposes.
        \item Names to store variables and constants' identifiers.
    \end{itemize}
    
    Semantic analysis constructs and utilizes the \textit{symbol table}, which is implemented as a "spaghetti stack" structure. Each item of the stack represents a \textit{scope}. 
    
    A \textit{scope} is a structure that contains:
    \begin{itemize}
        \item A Red-Black binary search tree (BST) for efficient symbol storage and lookup.
        \item A counter for unused variables within the scope.
    \end{itemize}
    
    Additionally, the symbol table structure includes a regular stack to facilitate freeing all allocated scopes when exiting a block or function.
    
    

	\subsection{Target code generation}

        \subsection*{Structure}

        Target code, (\texttt{IFJcode24}), is constructed using the AST and symtable. The final structure of a target program is the following:
        \begin{verbatim}
        .IFJcode24
        CALL main
        JUMP end_program
        LABEL main
            ... # main function's body
        LABEL f1
            DEFVAR LF@x
            DEFVAR LF@out
            ... # f1 function's body
            PUSHS LF@out
        LABEL end_program
        \end{verbatim}

        \subsection*{Algorithm}
        
        The main idea of the algorithm for generating the target code is the following.
        \begin{enumerate}
            \item Create labels in memory: \texttt{"main"} and \texttt{"end\_program"}.
            \item Output a call to label \texttt{"main"}.
            \item Output a \texttt{JUMP} to \texttt{"end\_program"} label.
            \item For each node in the AST with a \texttt{Function} type:
            \begin{enumerate}
                \item Create and print a label representing the function's entry point.
                \item For each statement within a function:
                \begin{enumerate}
                    \item Add appropriate instruction(s) to the function's instruction context.
                    \item If a new variable (including temporary) must be defined within the scope of the function, add it to the function's variables context.
                \end{enumerate}
                \item Output every variable definition within the function's variables context.
                \item Output every other instruction within the context.
            \end{enumerate}
            \item Output \texttt{"end\_program"} label.
        \end{enumerate}

        \subsection*{Development}

        When I began working on target generation, proper syntax and semantic analyses were not yet in place. As a result, the first sub-component I implemented was functionality for outputting instructions programmatically, which can be found in \texttt{instructions.h}. Following this, I started adding scaffolding to the \texttt{target\_gen.h} component, creating functions with comments describing the intended behavior but without actual implementations.
        
        After the AST was developed, the real work began. I implemented the outlined functions one by one, and during this process, I encountered two key challenges:
        
        \begin{enumerate}
            \item \texttt{DEFVAR} can be invoked only once per variable name within a scope. Redefining the same variable in a single scope causes an interpretation error. Since functions must operate within a single scope, this led to issues with variables defined inside loops in the AST. To resolve this, I grouped all \texttt{DEFVAR} instructions while traversing the AST and emitted them at the beginning of the function, before generating other instructions. This solution is implemented in \texttt{target\_func\_context.h}.
            \item To prevent variable name collisions, I adopted a method to generate unique identifiers by appending a distinct suffix to each variable or label. A separate indexer is used for labels. New indexer is created for each function scope. This functionality is implemented in \texttt{id\_indexer.h}.
            \item When most of the functionality was ready, I've realized that we could've used polish notation to simplify the generation of arithmetic operations. Unfortunately, due to time constraints, I've decided to not implement it.
        \end{enumerate}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conclusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Conclusion}

	The project initially surprised us a bit with its scope and complexity. Over time, until we had enough
	knowledge about compiler development from the IFJ lectures, we started to tackle the project.

	We had our team assembled very early on, and we had already agreed on the communication channels in advance,
	face-to-face meetings and the use of the versioning system, so we didn't have any
	and we worked very well together.

	We started working on the project a bit late, so we didn't have time to spare, but in the end
	we got everything done. We worked on the different parts of the project mostly individually using
	knowledge from lectures or materials for IFJ and IAL courses.

	During the development, we encountered minor problems related to ambiguities in the assignment, but
	these have been resolved through the project forum. The correctness of the solution was verified by automatic
	tests and ~test submissions, which allowed us to further fine-tune the project.

	Overall, this project has given us a lot of knowledge about how compilers work, practically
	clarified the subject matter covered in the IFJ and ~IAL courses, and~brought us experience with~projects of this magnitude.

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Attachments %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	\appendix

	\begin{figure}[!ht]
        \section{Finite state machine diagram specifying a lexical analyzer}
		\centering
		\includegraphics[width=0.8\linewidth]{FSM.pdf}
		\caption{Finite state machine diagram specifying a lexical analyzer}
		\label{figure:fa_graph}
	\end{figure}

	\begin{table}[!ht]
        \section{grammar}
		\centering
		\includegraphics[width=1\linewidth]{grammar1.pdf}
		\caption{grammar 1}
		\label{table:grammar part 1.pdf}
	\end{table}

    
	\begin{table}[!ht]
		\centering
		\includegraphics[width=0.7\linewidth]{grammar2.pdf}
		\caption{grammar 2}
		\label{table:grammar part 2.pdf}
	\end{table}

	\begin{table}[!ht]
        \section{LL -- table}
		\centering
		\includegraphics[width=1\linewidth]{LL_table.pdf}
		\caption{LL -- table used in syntactic analysis}
		\label{table:ll_table}
	\end{table}


	
	\begin{table}[!ht]
        \section{Precedence table}
		\centering
		\includegraphics[width=0.7\linewidth]{Precedence_table.pdf}
		\caption{Precedence table used in precedent syntactic analysis of expressions}
		\label{table:Precedence table}
	\end{table}


\end{document}